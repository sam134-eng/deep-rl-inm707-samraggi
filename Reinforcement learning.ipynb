{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b996f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install yfinance\n",
    "import yfinance as yf\n",
    "\n",
    "# Download Apple stock data for the last 5 years\n",
    "data = yf.download(\"AAPL\", period=\"5y\")\n",
    "\n",
    "# Save the data to a CSV file\n",
    "data.to_csv(\"AAPL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2579f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "data = yf.download(\"AAPL\", period=\"5y\")\n",
    "data.to_csv(\"AAPL.csv\")\n",
    "print(\"Downloaded and saved AAPL stock data ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190dad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"AAPL.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae5d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and prepare stock data\n",
    "df = pd.read_csv(\"AAPL.csv\")\n",
    "\n",
    "# Check column names to make sure you reference the correct ones\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176d3b9-19db-4f56-ab8a-4eac206d130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load and preprocess Apple stock market data\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Read the CSV file downloaded via yfinance\n",
    "df = pd.read_csv(\"AAPL.csv\")\n",
    "\n",
    "# Rename the 'Price' column to 'Date' (it contains date values)\n",
    "df.rename(columns={'Price': 'Date'}, inplace=True)\n",
    "\n",
    "# Convert 'Open' and 'Close' price columns from strings to numeric types\n",
    "# (in case they were read as text by mistake)\n",
    "df['Open'] = pd.to_numeric(df['Open'], errors='coerce')\n",
    "df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "\n",
    "# Remove any rows that have missing values in 'Open' or 'Close'\n",
    "df.dropna(subset=['Open', 'Close'], inplace=True)\n",
    "\n",
    "# Sort the dataset chronologically based on the 'Date' column\n",
    "df = df.sort_values(\"Date\")\n",
    "\n",
    "# Reset the row index (useful after sorting or dropping rows)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Calculate daily profit/loss by subtracting 'Open' from 'Close'\n",
    "df['profit'] = df['Close'] - df['Open']\n",
    "\n",
    "# Task 2: State Transition Function and Reward Table\n",
    "\n",
    "\n",
    "# Function to convert (profit, holding) into one of 4 discrete states\n",
    "def get_state(profit, holding):\n",
    "    \"\"\"\n",
    "    Maps the current market condition and holding status into 4 discrete states:\n",
    "    - s0: profit↑ and holding stock → state 0\n",
    "    - s1: profit↑ and not holding → state 1\n",
    "    - s2: profit↓ and holding → state 2\n",
    "    - s3: profit↓ and not holding → state 3\n",
    "    \"\"\"\n",
    "    if profit >= 0 and holding == 1:\n",
    "        return 0  # s0: price up, holding\n",
    "    elif profit >= 0 and holding == 0:\n",
    "        return 1  # s1: price up, not holding\n",
    "    elif profit < 0 and holding == 1:\n",
    "        return 2  # s2: price down, holding\n",
    "    else:\n",
    "        return 3  # s3: price down, not holding\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Define Reward Table for Q-Learning\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Reward matrix: shape = [states x actions]\n",
    "# Actions: 0 = hold, 1 = buy, 2 = sell\n",
    "# Rewards reflect market outcomes and transaction costs\n",
    "\n",
    "R = np.array([\n",
    "    [ 0.2,  np.nan,  3.0],    # s0: price↑, holding → hold (small reward), sell (high reward)\n",
    "    [-0.1,   0.2,     np.nan],# s1: price↑, not holding → buy (small reward), hold = missed opportunity\n",
    "    [-0.4,  np.nan, -2.5],    # s2: price↓, holding → hold (penalty), sell (bigger penalty)\n",
    "    [-0.1,  -0.2,     np.nan] # s3: price↓, not holding → buy (risky), hold = no change\n",
    "])\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487125aa-dcb4-4d4e-ae05-3d2d53cb2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Task 2: Define Reward Table and State Transition Function\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Reward table: rows = states (s0–s3), columns = actions (0 = hold, 1 = buy, 2 = sell)\n",
    "# Updated rewards to reflect:\n",
    "# - Profitable selling in a rising market\n",
    "# - Losses for selling in a down market\n",
    "# - Slight transaction cost for buying\n",
    "# - Cautious incentives for holding\n",
    "\n",
    "R = np.array([\n",
    "    [ 0.2,  np.nan,  3.0],   # s0: profit↑, holding → hold (small gain), sell (big profit)\n",
    "    [-0.1,   0.2,     np.nan], # s1: profit↑, not holding → buy (positive), hold = cost\n",
    "    [-0.4,  np.nan, -2.5],   # s2: price↓, holding → penalty for both holding/selling\n",
    "    [-0.1,  -0.2,     np.nan]  # s3: price↓, not holding → buying here is risky\n",
    "])\n",
    "print(\"Reward table R defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb2ced-e787-4796-9952-a260c81f0e83",
   "metadata": {},
   "source": [
    "The state transition function is deterministic in our simplified stock environment. Given the current state `s_t`, action `a_t`, and the observed stock profit (close - open) on the next day, we determine `s_{t+1}` using the function:\n",
    "\n",
    "    s_{t+1} = get_state(profit[t+1], holding)\n",
    "\n",
    "The state is determined by:\n",
    "- The price movement direction (profit ≥ 0 or < 0)\n",
    "- Whether the agent is holding a stock (holding = 1 or 0) after taking action `a_t`\n",
    "\n",
    "Thus, the transition is deterministic and driven by sequential access to the time-series data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d6b01-e0de-4028-a816-68de7c106fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Task 2: Reward Table Heatmap \n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(R, annot=True, fmt=\".1f\", cmap=\"coolwarm_r\", cbar=True,\n",
    "            xticklabels=['Hold/Nothing', 'Buy', 'Sell'],\n",
    "            yticklabels=['s0 (P>=0, H=1)', 's1 (P>=0, H=0)',\n",
    "                         's2 (P<0, H=1)', 's3 (P<0, H=0)'])\n",
    "plt.title('Reward Table (R)')\n",
    "plt.xlabel('Actions')\n",
    "plt.ylabel('States')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e73ce-e668-40aa-981b-a4b8a90d867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the get_state function (this should convert profit and holding into a state)\n",
    "def get_state(profit, holding):\n",
    "    # Simplified state representation based on profit and holding (you can adjust this)\n",
    "    return min(int(profit // 10), 3)  # Example: state ranges from 0 to 3 based on profit\n",
    "\n",
    "# Define the take_action function (this simulates the environment's response to the action)\n",
    "def take_action(state, action, profit, budget):\n",
    "    \"\"\"\n",
    "    This function simulates the environment's response to the agent's action.\n",
    "    It returns the next state and the reward (which can be the profit or loss).\n",
    "    \"\"\"\n",
    "    if action == 0:  # Example: Buy action\n",
    "        reward = profit * 0.8  # Assume 80% of profit is gained when buying\n",
    "        next_state = (state + 1) % 4  # Transition to a new state based on action\n",
    "    elif action == 1:  # Example: Sell action\n",
    "        reward = profit * 1.2  # Assume 120% of profit when selling\n",
    "        next_state = (state + 2) % 4  # Transition to a new state based on action\n",
    "    else:  # Example: Hold action\n",
    "        reward = 0  # No reward for holding\n",
    "        next_state = state  # Stay in the same state for holding\n",
    "\n",
    "    # Update the budget\n",
    "    budget -= reward  # Example: budget decreases by the reward value\n",
    "\n",
    "    return next_state, reward\n",
    "\n",
    "# Q-learning algorithm\n",
    "def q_learning(df, alpha, gamma, epsilon_decay, num_episodes=1500, episode_length=300):\n",
    "    Q = np.zeros((4, 3))  # Initialize Q-table: 4 states x 3 actions\n",
    "    epsilon = 0.9         # Initial exploration rate\n",
    "    q_means = []          # For storing average Q-values each episode\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        # Random starting point\n",
    "        i = np.random.randint(len(df) - 300, len(df) - 1)\n",
    "        holding = np.random.randint(0, 2)\n",
    "        profit = df.iloc[i]['profit']\n",
    "        state = get_state(profit, holding)\n",
    "        budget = 100\n",
    "\n",
    "        for t in range(episode_length):\n",
    "            # Available actions based on the current state\n",
    "            available_actions = [0, 1, 2]  # Define valid actions (e.g., 0=Buy, 1=Sell, 2=Hold)\n",
    "\n",
    "            # ε-greedy action selection\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(available_actions)  # Exploration\n",
    "            else:\n",
    "                action = np.argmax(Q[state, available_actions])  # Exploitation\n",
    "\n",
    "            # Take action and observe next state and reward\n",
    "            next_state, reward = take_action(state, action, profit, budget)  # Call the defined take_action function\n",
    "            Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "\n",
    "            state = next_state  # Update state for next step\n",
    "\n",
    "        # Epsilon decay\n",
    "        if epsilon >= 0.5:\n",
    "            epsilon *= epsilon_decay\n",
    "        else:\n",
    "            epsilon *= 0.999\n",
    "        epsilon = max(epsilon, 0.01)  # Ensure epsilon doesn't go below 0.01\n",
    "\n",
    "        # Track the average Q-value of the Q-table for this episode\n",
    "        q_means.append(np.nanmean(Q))  # You can use np.nanmean to avoid issues with NaNs\n",
    "\n",
    "    return Q, q_means\n",
    "\n",
    "# Example values for the base experiment\n",
    "alpha_base = 0.1  # Example value\n",
    "gamma_base = 0.9  # Example value\n",
    "epsilon_decay_base = 0.99  # Example value\n",
    "num_episodes_base = 1500  # Example value\n",
    "episode_len_base = 300  # Example value\n",
    "\n",
    "# Assuming df_training_data is your DataFrame with the necessary data\n",
    "df_training_data = pd.DataFrame({\n",
    "    'profit': np.random.rand(1000) * 100  # Example: random profits (replace with your actual data)\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb8086-08b1-4dcd-83f0-c3d6c93e8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Task 3: Epsilon Decay Schedule ---\n",
    "epsilons_over_time = []\n",
    "epsilon_temp_plot = 0.9  # Initial epsilon for plotting\n",
    "epsilon_decay_plot = 0.9995 # Epsilon decay for the base case (can be changed to match specific experiments)\n",
    "num_episodes_plot = 1500 # Match the number of episodes in training\n",
    "\n",
    "for ep_plot in range(num_episodes_plot):\n",
    "    epsilons_over_time.append(epsilon_temp_plot)\n",
    "    # Mirrored the decay logic from q_learning for consistency\n",
    "    if epsilon_temp_plot >= 0.1: \n",
    "        epsilon_temp_plot *= epsilon_decay_plot\n",
    "    else:\n",
    "        epsilon_temp_plot *= 0.999 # Slower decay for lower epsilon\n",
    "    epsilon_temp_plot = max(epsilon_temp_plot, 0.01)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epsilons_over_time)\n",
    "plt.title('Epsilon Decay Over Episodes (Example Schedule)')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Epsilon Value')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1be46-046a-4a4e-9639-2f35f6d49dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Q-learning for the base experiment\n",
    "print(f\"Starting Base Experiment (α={alpha_base}, γ={gamma_base})...\")\n",
    "Q_base, q_mean_base = q_learning(\n",
    "    df_training_data,  \n",
    "    alpha_base,\n",
    "    gamma_base,\n",
    "    epsilon_decay_base,\n",
    "    num_episodes=num_episodes_base,\n",
    "    episode_length=episode_len_base\n",
    ")\n",
    "print(\"Base Experiment Training Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43992ef-dbf8-46bd-ac25-fa5484700383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------\n",
    "# Define hyperparameters for the base experiment\n",
    "# ---------------------------------------\n",
    "\n",
    "alpha_base = 0.6\n",
    "gamma_base = 0.6\n",
    "epsilon_decay_base = 0.9995\n",
    "num_episodes_base = 1500      # Total episodes for training\n",
    "episode_len_base = 300        # Steps per episode\n",
    "\n",
    "# ---------------------------------------\n",
    "# Split the dataset into training and evaluation sets\n",
    "# ---------------------------------------\n",
    "\n",
    "train_split_index = int(len(df) * 0.8)\n",
    "df_training_data = df.iloc[:train_split_index].reset_index(drop=True)\n",
    "df_evaluation_data = df.iloc[train_split_index:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training data length: {len(df_training_data)}\")\n",
    "print(f\"Evaluation data length: {len(df_evaluation_data)}\")\n",
    "\n",
    "if len(df_training_data) < episode_len_base + 1:\n",
    "    print(f\"Warning: Training data (length {len(df_training_data)}) is too short for the episode length ({episode_len_base}).\")\n",
    "    # You can reduce episode_len_base here if needed.\n",
    "\n",
    "# ---------------------------------------\n",
    "# Q-learning function with reward tracking\n",
    "# ---------------------------------------\n",
    "\n",
    "def q_learning(df, alpha, gamma, epsilon_decay, num_episodes=1500, episode_length=300):\n",
    "    Q = np.zeros((4, 3))  # Q-table: 4 states × 3 actions\n",
    "    epsilon = 0.9         # Initial exploration rate\n",
    "    q_means = []          # Track average Q-values per episode\n",
    "    episode_rewards = []  # Track total reward per episode\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        i = np.random.randint(len(df) - 300, len(df) - 1)\n",
    "        holding = np.random.randint(0, 2)\n",
    "        profit = df.iloc[i]['profit']\n",
    "        state = get_state(profit, holding)\n",
    "        budget = 100\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(episode_length):\n",
    "            available_actions = [0, 1, 2]  # buy, sell, hold\n",
    "\n",
    "            # ε-greedy action selection\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(available_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q[state, available_actions])\n",
    "\n",
    "            # Take action and observe next state and reward\n",
    "            next_state, reward = take_action(state, action, profit, budget)\n",
    "            total_reward += reward  # Accumulate total reward for this episode\n",
    "\n",
    "            # Q-learning update\n",
    "            Q[state, action] = Q[state, action] + alpha * (\n",
    "                reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "\n",
    "            state = next_state  # Update state\n",
    "\n",
    "        # Decay epsilon (exploration rate)\n",
    "        if epsilon >= 0.5:\n",
    "            epsilon *= epsilon_decay\n",
    "        else:\n",
    "            epsilon *= 0.999\n",
    "        epsilon = max(epsilon, 0.01)\n",
    "\n",
    "        q_means.append(np.nanmean(Q))\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "    return Q, q_means, episode_rewards\n",
    "\n",
    "# ---------------------------------------\n",
    "# Run Q-learning base experiment\n",
    "# ---------------------------------------\n",
    "\n",
    "print(f\"Starting Base Experiment (α={alpha_base}, γ={gamma_base})...\")\n",
    "Q_base, q_mean_base, rewards_base = q_learning(\n",
    "    df_training_data,\n",
    "    alpha_base,\n",
    "    gamma_base,\n",
    "    epsilon_decay_base,\n",
    "    num_episodes=num_episodes_base,\n",
    "    episode_length=episode_len_base\n",
    ")\n",
    "print(\"Base Experiment Training Completed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910007e-e308-499b-9df2-e649d7f1bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Average Q-value convergence\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(q_mean_base)\n",
    "plt.title(f\"Q-value Convergence in Base Experiment (α={alpha_base}, γ={gamma_base})\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Q-value in Q-Table\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Cumulative Reward per Episode\n",
    "# ---------------------------------------\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rewards_base)\n",
    "plt.title(f\"Cumulative Reward per Episode (Base Experiment, α={alpha_base}, γ={gamma_base})\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward per Episode\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cb007-545c-430f-bd04-4f01296ffccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot 3: Final Q-Table Heatmap (Base Experiment)\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(Q_base, annot=True, fmt=\".2f\", cmap=\"viridis\", cbar=True,\n",
    "            xticklabels=['Hold/Nothing', 'Buy', 'Sell'],\n",
    "            yticklabels=['s0 (P>=0,H=1)', 's1 (P>=0,H=0)', 's2 (P<0,H=1)', 's3 (P<0,H=0)'])\n",
    "plt.title(f\"Final Q-Table (Base Experiment, α={alpha_base}, γ={gamma_base})\")\n",
    "plt.xlabel('Actions')\n",
    "plt.ylabel('States')\n",
    "plt.show()\n",
    "\n",
    "# Output 4: Learned Policy Visualization (Base Experiment)\n",
    "Q_base_for_policy = np.nan_to_num(Q_base.copy(), nan=-np.inf) # Replace nan with -inf for argmax\n",
    "policy_base = np.argmax(Q_base_for_policy, axis=1)\n",
    "action_map = {0: 'Hold/Nothing', 1: 'Buy', 2: 'Sell'}\n",
    "state_map_display = {0: 's0 (Profit>=0, Holding)', 1: 's1 (Profit>=0, Not Holding)',\n",
    "                     2: 's2 (Profit<0, Holding)', 3: 's3 (Profit<0, Not Holding)'}\n",
    "\n",
    "print(\"\\nLearned Policy (Base Experiment):\")\n",
    "for s, a_idx in enumerate(policy_base):\n",
    "    if not np.isnan(R[s, a_idx]): # Check if the chosen action is valid for the state\n",
    "        print(f\"State {state_map_display[s]}: Action {action_map[a_idx]} (Q-value: {Q_base[s, a_idx]:.3f})\")\n",
    "    else:\n",
    "        # Fallback if argmax on Q_base_for_policy somehow picked an action marked as NaN in R\n",
    "        valid_actions_for_state = np.where(~np.isnan(R[s]))[0]\n",
    "        if len(valid_actions_for_state) > 0:\n",
    "            q_vals_valid = Q_base[s, valid_actions_for_state]\n",
    "            best_valid_action_local_idx = np.argmax(q_vals_valid)\n",
    "            best_valid_action_global_idx = valid_actions_for_state[best_valid_action_local_idx]\n",
    "            print(f\"State {state_map_display[s]}: Action {action_map[best_valid_action_global_idx]} (Q-value: {Q_base[s, best_valid_action_global_idx]:.3f}) [Best Valid Action]\")\n",
    "        else:\n",
    "            print(f\"State {state_map_display[s]}: No valid actions defined in R table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f292f-6e3a-40d2-8836-62a110f97ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 5: Experiment with Different Parameters ---\n",
    "\n",
    "# Define different configurations of (alpha, gamma, label_for_plot)\n",
    "configs = [(0.3, 0.6, 'α=0.3, γ=0.6'), (0.6, 0.3, 'α=0.6, γ=0.3'),\n",
    "           (0.6, 0.9, 'α=0.6, γ=0.9'), (0.9, 0.6, 'α=0.9, γ=0.6')]\n",
    "\n",
    "# Store results for comparison\n",
    "all_q_means_config = {}\n",
    "all_rewards_config = {}\n",
    "all_Q_tables_config = {} # To store Q-tables for later evaluation\n",
    "\n",
    "epsilon_decay_configs = 0.9995 # Keep epsilon decay constant for this comparison\n",
    "num_episodes_configs = 1500    # Number of episodes for these experiments\n",
    "episode_len_configs = 300      # Episode length\n",
    "\n",
    "print(\"Starting Parameter Experiments...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192f26f-67c5-4a8f-88d0-ebd37386714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figures for plotting before the loop\n",
    "fig_q_convergence, ax_q_convergence = plt.subplots(figsize=(12, 6))\n",
    "ax_q_convergence.set_title(\"Comparison of Q-value Convergence Across Parameters\")\n",
    "ax_q_convergence.set_xlabel(\"Episodes\")\n",
    "ax_q_convergence.set_ylabel(\"Average Q-value in Q-Table\")\n",
    "ax_q_convergence.grid(True)\n",
    "\n",
    "fig_reward_convergence, ax_reward_convergence = plt.subplots(figsize=(12, 6))\n",
    "ax_reward_convergence.set_title(\"Comparison of Cumulative Reward Convergence Across Parameters\")\n",
    "ax_reward_convergence.set_xlabel(\"Episodes\")\n",
    "ax_reward_convergence.set_ylabel(\"Total Reward from R table per Episode\")\n",
    "ax_reward_convergence.grid(True)\n",
    "\n",
    "for alpha_conf, gamma_conf, label in configs:\n",
    "    print(f\"\\nRunning experiment with: {label}\")\n",
    "    Q_conf, q_mean_conf, rewards_conf = q_learning(\n",
    "        df_training_data, \n",
    "        alpha_conf, \n",
    "        gamma_conf, \n",
    "        epsilon_decay_configs, \n",
    "        num_episodes=num_episodes_configs,\n",
    "        episode_length=episode_len_configs\n",
    "    )\n",
    "    \n",
    "    all_q_means_config[label] = q_mean_conf\n",
    "    all_rewards_config[label] = rewards_conf\n",
    "    all_Q_tables_config[label] = Q_conf # Store the Q-table\n",
    "    \n",
    "    ax_q_convergence.plot(q_mean_conf, label=label)\n",
    "    ax_reward_convergence.plot(rewards_conf, label=label)\n",
    "\n",
    "# Add legends to the plots\n",
    "ax_q_convergence.legend()\n",
    "fig_q_convergence.show() # Or plt.show() if not using %matplotlib inline magic for interactive backends\n",
    "\n",
    "ax_reward_convergence.legend()\n",
    "fig_reward_convergence.show() # Or plt.show()\n",
    "\n",
    "print(\"Parameter Experiments Training Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1a86f-8b98-4a32-aaa1-a89c224c0998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 6: Qualitative & Quantitative Analysis ---\n",
    "\n",
    "# Function to evaluate the learned policy's actual financial performance\n",
    "def evaluate_policy_financial(Q_table_eval, eval_data, initial_budget=10000, episode_len_eval=None):\n",
    "    \"\"\"\n",
    "    Evaluates the policy based on a Q-table on evaluation data, tracking actual financial profit.\n",
    "    Args:\n",
    "        Q_table_eval (np.array): The trained Q-table.\n",
    "        eval_data (pd.DataFrame): DataFrame for evaluation (unseen during training ideally).\n",
    "        initial_budget (float): Starting budget for evaluation.\n",
    "        episode_len_eval (int, optional): Max number of steps for this evaluation run. If None, runs through all eval_data.\n",
    "    Returns:\n",
    "        tuple: (actual_profit, final_budget, trade_details_list, budget_history, portfolio_value_history)\n",
    "    \"\"\"\n",
    "    current_holding = 0  # Start evaluation without holding stock\n",
    "    budget = initial_budget\n",
    "    current_day_idx = 0\n",
    "    \n",
    "    trade_details_list = [] \n",
    "    budget_history = [initial_budget]\n",
    "    portfolio_value_history = [initial_budget] \n",
    "\n",
    "    if episode_len_eval is None:\n",
    "        episode_len_eval = len(eval_data) - 1 \n",
    "    else:\n",
    "        episode_len_eval = min(episode_len_eval, len(eval_data) - 1)\n",
    "\n",
    "    last_buy_price = 0 \n",
    "\n",
    "    for t_eval in range(episode_len_eval):\n",
    "        if current_day_idx >= len(eval_data) -1: \n",
    "            break\n",
    "\n",
    "        profit_metric_today = eval_data.iloc[current_day_idx]['profit']\n",
    "        current_eval_state = get_state(profit_metric_today, current_holding)\n",
    "\n",
    "        available_actions_eval = np.where(~np.isnan(R[current_eval_state]))[0]\n",
    "        \n",
    "        chosen_action_eval = 0 \n",
    "        if len(available_actions_eval) > 0:\n",
    "            # Greedily select action (epsilon=0 for evaluation)\n",
    "            # Handle NaNs in Q_table_eval for unavailable actions if they weren't set to -inf before argmax\n",
    "            q_values_for_eval_state_available = Q_table_eval[current_eval_state, available_actions_eval]\n",
    "            action_index_in_available_eval = np.argmax(q_values_for_eval_state_available)\n",
    "            chosen_action_eval = available_actions_eval[action_index_in_available_eval]\n",
    "        \n",
    "        current_stock_price = eval_data.iloc[current_day_idx]['Close']\n",
    "\n",
    "        if chosen_action_eval == 1 and current_holding == 0:  # Buy\n",
    "            if budget >= current_stock_price:\n",
    "                current_holding = 1\n",
    "                budget -= current_stock_price\n",
    "                last_buy_price = current_stock_price\n",
    "                trade_details_list.append({'Day': eval_data['Date'].iloc[current_day_idx], 'Action': 'Buy', 'Price': current_stock_price, 'Shares': 1, 'Budget': budget})\n",
    "        elif chosen_action_eval == 2 and current_holding == 1:  # Sell\n",
    "            current_holding = 0\n",
    "            budget += current_stock_price\n",
    "            profit_this_trade = current_stock_price - last_buy_price if last_buy_price > 0 else 0\n",
    "            trade_details_list.append({'Day': eval_data['Date'].iloc[current_day_idx], 'Action': 'Sell', 'Price': current_stock_price, 'Shares': 1, 'Profit_Trade': profit_this_trade, 'Budget': budget})\n",
    "            last_buy_price = 0 \n",
    "\n",
    "        budget_history.append(budget)\n",
    "        \n",
    "        current_portfolio_value = budget\n",
    "        if current_holding == 1:\n",
    "            # Use current day's close to value holding stock for portfolio calculation\n",
    "            current_portfolio_value += eval_data.iloc[current_day_idx]['Close'] \n",
    "        portfolio_value_history.append(current_portfolio_value)\n",
    "\n",
    "        current_day_idx += 1\n",
    "\n",
    "    final_budget = budget\n",
    "    if current_holding == 1: # Liquidate if still holding at the end\n",
    "        final_price = eval_data.iloc[current_day_idx -1]['Close'] \n",
    "        final_budget += final_price\n",
    "        profit_this_trade = final_price - last_buy_price if last_buy_price > 0 else 0\n",
    "        trade_details_list.append({'Day': eval_data['Date'].iloc[current_day_idx-1], 'Action': 'Liquidate_Sell', 'Price': final_price, 'Shares': 1, 'Profit_Trade': profit_this_trade, 'Budget': final_budget})\n",
    "        # Update last entry in histories if needed, or ensure they reflect final state\n",
    "        if budget_history[-1] != final_budget: budget_history.append(final_budget)\n",
    "        if portfolio_value_history[-1] != final_budget : portfolio_value_history.append(final_budget)\n",
    "\n",
    "\n",
    "    actual_profit = final_budget - initial_budget\n",
    "    return actual_profit, final_budget, trade_details_list, budget_history, portfolio_value_history\n",
    "\n",
    "print(\"evaluate_policy_financial function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5653d1-bd6c-4bb5-a0b0-61e058e3f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Perform Financial Evaluation for the Base Model ---\n",
    "print(f\"\\n--- Financial Evaluation on Unseen Data (using {len(df_evaluation_data)} days) ---\")\n",
    "\n",
    "# Ensure df_evaluation_data is not empty and has enough data\n",
    "if len(df_evaluation_data) > 1: \n",
    "    actual_profit_base_eval, final_budget_base_eval, trades_base_eval, budget_hist_base, port_val_hist_base = \\\n",
    "        evaluate_policy_financial(Q_base, df_evaluation_data.copy()) # Use .copy() \n",
    "\n",
    "    print(f\"\\nBase Policy (α={alpha_base}, γ={gamma_base}) Financial Evaluation:\")\n",
    "    print(f\"  Initial Budget: 10000\") # Assuming 10000 from function default\n",
    "    print(f\"  Actual Profit on Evaluation Data: {actual_profit_base_eval:.2f}\")\n",
    "    print(f\"  Final Budget after Evaluation: {final_budget_base_eval:.2f}\")\n",
    "    print(f\"  Number of Trades: {len([t for t in trades_base_eval if t['Action'] in ['Buy', 'Sell']])}\")\n",
    "\n",
    "    # Plot Budget Over Time during Evaluation\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(budget_hist_base, label=\"Budget Over Time\")\n",
    "    plt.title(f\"Budget During Financial Evaluation (Base Policy α={alpha_base}, γ={gamma_base})\")\n",
    "    plt.xlabel(\"Trading Days in Evaluation Period\")\n",
    "    plt.ylabel(\"Budget\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Portfolio Value Over Time during Evaluation\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(port_val_hist_base, label=\"Portfolio Value Over Time\", color='green')\n",
    "    plt.title(f\"Portfolio Value During Financial Evaluation (Base Policy α={alpha_base}, γ={gamma_base})\")\n",
    "    plt.xlabel(\"Trading Days in Evaluation Period\")\n",
    "    plt.ylabel(\"Portfolio Value (Budget + Stock Value if Held)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Plot trades on stock price chart for a segment of evaluation\n",
    "    if trades_base_eval:\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        # Ensure eval_price_data matches the length of decisions made (up to episode_len_eval)\n",
    "        num_eval_days_plotted = min(len(df_evaluation_data), len(port_val_hist_base) -1) # -1 because history has initial state\n",
    "        eval_price_data = df_evaluation_data['Close'].values[:num_eval_days_plotted]\n",
    "        \n",
    "        plt.plot(range(num_eval_days_plotted), eval_price_data, label='Stock Close Price', alpha=0.7)\n",
    "\n",
    "        # Adjust trade indices relative to the plotted segment\n",
    "        buy_indices = []\n",
    "        sell_indices = []\n",
    "        \n",
    "        # Get day indices from trade_details_list based on eval_data's original index\n",
    "        for trade in trades_base_eval:\n",
    "            try:\n",
    "                # Find the index of the trade day within the df_evaluation_data\n",
    "                trade_day_original_index = df_evaluation_data[df_evaluation_data['Date'] == trade['Day']].index[0]\n",
    "                if trade_day_original_index < num_eval_days_plotted: # Only plot if within the plotted range\n",
    "                    if trade['Action'] == 'Buy':\n",
    "                        buy_indices.append(trade_day_original_index)\n",
    "                    elif trade['Action'] in ['Sell', 'Liquidate_Sell']:\n",
    "                        sell_indices.append(trade_day_original_index)\n",
    "            except IndexError:\n",
    "                # Date not found, or other issue; skip this trade for plotting\n",
    "                pass\n",
    "\n",
    "        if buy_indices:\n",
    "             plt.scatter(buy_indices, eval_price_data[buy_indices], marker='^', color='green', s=100, label='Buy', zorder=5)\n",
    "        if sell_indices:\n",
    "             plt.scatter(sell_indices, eval_price_data[sell_indices], marker='v', color='red', s=100, label='Sell', zorder=5)\n",
    "        \n",
    "        plt.title(\"Stock Price with Trades During Evaluation (Base Policy)\")\n",
    "        plt.xlabel(f\"Trading Days (0 to {num_eval_days_plotted-1}) in Evaluation Period\")\n",
    "        plt.ylabel(\"Stock Price\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Evaluation data is too short to perform financial evaluation for the base model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0aa29e-d162-4eca-bddf-9f418c7154be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Financial Evaluation for Other Configurations (Task 5 models) ---\n",
    "print(\"\\n--- Financial Evaluation for Different Alpha/Gamma Configurations ---\")\n",
    "\n",
    "# Collect results for a summary table or bar chart\n",
    "eval_results_summary = []\n",
    "\n",
    "for alpha_c, gamma_c, label_c in configs:\n",
    "    print(f\"\\nEvaluating Policy ({label_c})...\")\n",
    "    Q_to_eval = all_Q_tables_config.get(label_c) # Get the stored Q-table\n",
    "    \n",
    "    if Q_to_eval is None:\n",
    "        print(f\"Q-table for {label_c} not found. Skipping evaluation.\")\n",
    "        continue\n",
    "        \n",
    "    if len(df_evaluation_data) > 1:\n",
    "        actual_profit_conf_eval, final_budget_conf_eval, trades_conf_eval, _, port_val_hist_conf = \\\n",
    "            evaluate_policy_financial(Q_to_eval, df_evaluation_data.copy())\n",
    "        \n",
    "        print(f\"  Policy ({label_c}) Financial Evaluation:\")\n",
    "        print(f\"  Actual Profit on Evaluation Data: {actual_profit_conf_eval:.2f}\")\n",
    "        print(f\"  Final Budget after Evaluation: {final_budget_conf_eval:.2f}\")\n",
    "        print(f\"  Number of Trades: {len([t for t in trades_conf_eval if t['Action'] in ['Buy', 'Sell']])}\")\n",
    "        eval_results_summary.append({'Config': label_c, 'Profit': actual_profit_conf_eval, 'FinalBudget': final_budget_conf_eval})\n",
    "\n",
    "        # Optional: Plot portfolio value for each config\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(port_val_hist_conf, label=f\"Portfolio Value ({label_c})\")\n",
    "        plt.title(f\"Portfolio Value During Financial Evaluation ({label_c})\")\n",
    "        plt.xlabel(\"Trading Days in Evaluation Period\")\n",
    "        plt.ylabel(\"Portfolio Value\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Skipping financial evaluation for {label_c} due to insufficient evaluation data.\")\n",
    "\n",
    "# Optional: Display summary of financial evaluation results\n",
    "if eval_results_summary:\n",
    "    df_eval_summary = pd.DataFrame(eval_results_summary)\n",
    "    print(\"\\n--- Summary of Financial Evaluation Results ---\")\n",
    "    print(df_eval_summary)\n",
    "    \n",
    "    # Bar chart for comparison\n",
    "    df_eval_summary.set_index('Config').plot(kind='bar', y=['Profit', 'FinalBudget'], \n",
    "                                             figsize=(10,6), secondary_y='FinalBudget')\n",
    "    plt.title(\"Comparison of Financial Performance Across Configurations\")\n",
    "    plt.ylabel(\"Actual Profit\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c30feb-9829-4554-b56a-3071e7ba8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import TensorFlow and Keras for building the neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64cc424-7ca1-4426-8c2f-97dbd2fbb16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input  \n",
    "\n",
    "def build_q_network(state_size, action_size):\n",
    "    \"\"\"\n",
    "    Builds a simple MLP using the modern Keras Input layer syntax.\n",
    "    \n",
    "    Parameters:\n",
    "        state_size (int): Dimension of input state space\n",
    "        action_size (int): Number of possible actions\n",
    "\n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(state_size,)),             # Modern Input layer usage\n",
    "        Dense(24, activation='relu'),\n",
    "        Dense(24, activation='relu'),\n",
    "        Dense(action_size, activation='linear')  # Linear output for Q-values\n",
    "    ])\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2833b6-f538-4d9b-88f3-d0079c8823ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the DQN Agent with Double DQN and Experience Replay\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size  # e.g., 4 for CartPole\n",
    "        self.action_size = action_size  # e.g., 2 actions: left or right\n",
    "        self.memory = deque(maxlen=2000)  # Experience replay buffer\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01  # Minimum epsilon\n",
    "        self.epsilon_decay = 0.995  # Decay rate for epsilon\n",
    "        self.batch_size = 64  # Mini-batch size\n",
    "        self.train_start = 1000  # Start training only after this many experiences\n",
    "\n",
    "        # Build two models: policy network (used to choose actions) and target network (used to calculate Q-values)\n",
    "        self.model = build_q_network(state_size, action_size)\n",
    "        self.target_model = build_q_network(state_size, action_size)\n",
    "        self.update_target_model()  # Synchronize target model initially\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Copy weights from policy model to target model\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store the experience in replay buffer\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        q_values = self.model.predict(state, verbose=0)  # Exploit\n",
    "        return np.argmax(q_values[0])  # Return action with highest Q-value\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Train the policy network using a random batch from replay memory\"\"\"\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        next_states = np.zeros((self.batch_size, self.state_size))\n",
    "        actions, rewards, dones = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            states[i] = minibatch[i][0]\n",
    "            actions.append(minibatch[i][1])\n",
    "            rewards.append(minibatch[i][2])\n",
    "            next_states[i] = minibatch[i][3]\n",
    "            dones.append(minibatch[i][4])\n",
    "\n",
    "        # Predict Q-values for next states using policy network and target network (Double DQN)\n",
    "        target = self.model.predict(states, verbose=0)\n",
    "        target_next = self.model.predict(next_states, verbose=0)\n",
    "        target_val = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                # Double DQN: use policy model to get best action, use target model to evaluate it\n",
    "                best_action = np.argmax(target_next[i])\n",
    "                target[i][actions[i]] = rewards[i] + self.gamma * target_val[i][best_action]\n",
    "\n",
    "        # Train the model\n",
    "        self.model.fit(states, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "        # Decay epsilon after each training step\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc30dc-7a7c-42e7-b1f8-f557a1527682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Agent on CartPole-v1\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "episodes = 1000\n",
    "target_update_interval = 100  # Update target network every 100 episodes\n",
    "scores, avg_scores = [], []\n",
    "\n",
    "for e in range(episodes):\n",
    "    state, _ = env.reset()  # Modern Gym API: returns (obs, info)\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)  # Choose action based on policy\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)  # Modern Gym API\n",
    "        done = terminated or truncated  # Done flag based on either\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        agent.replay()\n",
    "\n",
    "    scores.append(total_reward)\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    avg_scores.append(avg_score)\n",
    "\n",
    "    if e % target_update_interval == 0:\n",
    "        agent.update_target_model()\n",
    "\n",
    "    print(f\"Episode {e+1}, Reward: {total_reward}, Avg Reward (100): {avg_score:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b83699-ca87-4b1e-871a-25bd98da7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total rewards and average rewards\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(scores, label='Total Reward per Episode')\n",
    "plt.plot(avg_scores, label='Average Reward over 100 Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('DQN Agent Performance on CartPole-v1')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
